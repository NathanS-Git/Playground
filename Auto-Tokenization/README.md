It is a problem that networks need tokenized inputs, be it images or text, built up from small parts to more general parts like words or sub-images. These sub-components are rigid and defined by the tokenizer. What if they could be more general? If all 5's looked the same and took up the same amount of pixels, then tokenization would be great, but not all 5's look the same, be them in different fonts, or drawn. Ideally a tokenizer could tokenize them all as 5's. This example I'm more so talking about pixel/image tokenization - but this would apply to words as well. How would we build a model (probably a neural network based one) that builds up these tokenizations of abstract concepts? Is an MNIST digit classifier already one of these special tokenizers I'm talking about? Not quite, but close I think..
